{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#뉴스 선정(GoogleNews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 종목의 일자별 뉴스 검색.\n",
    "내용 관련성 -> 특정 키워드 기반 필터링\n",
    "유사성 -> TF-IDF(흔한 단어는 가중치 낮추고 중요한 단어는 가중치 높임) 및 코사인 유사도(두 문서가 얼마나 유사한지 측정해 중복된 뉴스를 제거) 기반 분석\n",
    "최대 5개 뉴스로 제한, 부족 시 추가 확보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GoogleNews\n",
      "  Downloading GoogleNews-1.6.15-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from GoogleNews) (4.12.2)\n",
      "Collecting dateparser (from GoogleNews)\n",
      "  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: python-dateutil in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from GoogleNews) (2.9.0.post0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from beautifulsoup4->GoogleNews) (2.5)\n",
      "Collecting pytz>=2024.2 (from dateparser->GoogleNews)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dateparser->GoogleNews) (2023.10.3)\n",
      "Collecting tzlocal>=0.2 (from dateparser->GoogleNews)\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil->GoogleNews) (1.16.0)\n",
      "Downloading GoogleNews-1.6.15-py3-none-any.whl (8.8 kB)\n",
      "Downloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: pytz, tzlocal, dateparser, GoogleNews\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.3.post1\n",
      "    Uninstalling pytz-2023.3.post1:\n",
      "      Successfully uninstalled pytz-2023.3.post1\n",
      "Successfully installed GoogleNews-1.6.15 dateparser-1.2.1 pytz-2025.1 tzlocal-5.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install GoogleNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. 라이브러리 임포트\n",
    "\n",
    "%pip install yfinance  # (Colab 등에서 필요 시)\n",
    "%pip install tensorflow\n",
    "%pip install matplot\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 데이터 수집 (주가 + 임의 감정 점수)\n",
    "import yfinance as yf\n",
    "\n",
    "ticker = 'AAPL'  # 애플 예시\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2025-01-01'\n",
    "\n",
    "df = yf.download(ticker, start=start_date, end=end_date)\n",
    "# df: ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "df.dropna(inplace=True)\n",
    "# 감정분석 값 추가 (기본값 = 0)\n",
    "df['Sentiment'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 4. 멀티피처 (Close, Volume, Sentiment) + 정규화\n",
    "###################################\n",
    "feature_cols = ['Close', 'Volume', 'Sentiment']\n",
    "data = df[feature_cols].copy()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# DataFrame 형태로 다시 저장(편의용)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=feature_cols, index=df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 5. 시계열 윈도우 생성 함수\n",
    "###################################\n",
    "def create_sequences_multi_features(scaled_df, window_size=7, target_col='Close'):\n",
    "    \"\"\"\n",
    "    window_size일 동안의 [Close,Volume,Sentiment]를 보고 \n",
    "    다음 날(target_col)의 값을 예측하는 구조.\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    dates = scaled_df.index\n",
    "    date_list = []\n",
    "\n",
    "    target_idx = scaled_df.columns.get_loc(target_col)\n",
    "\n",
    "    for i in range(len(scaled_df) - window_size):\n",
    "        X_window = scaled_df.iloc[i : i + window_size].values\n",
    "        y_value = scaled_df.iloc[i + window_size, target_idx]\n",
    "        \n",
    "        X_list.append(X_window)\n",
    "        y_list.append(y_value)\n",
    "        date_list.append(dates[i + window_size])  # 이 샘플의 '정답' 날짜(예: 예측 시점)\n",
    "\n",
    "    X_arr = np.array(X_list)\n",
    "    y_arr = np.array(y_list)\n",
    "    return X_arr, y_arr, date_list\n",
    "\n",
    "window_size = 30\n",
    "X_all, y_all, date_all = create_sequences_multi_features(scaled_df, window_size=window_size, target_col='Close')\n",
    "\n",
    "print(\"X_all shape:\", X_all.shape)  # 예: (N, 30, 3)\n",
    "print(\"y_all shape:\", y_all.shape)  # 예: (N,)\n",
    "\n",
    "###################################\n",
    "# 6. Walk-Forward Validation 설정\n",
    "###################################\n",
    "def walk_forward_validation(X, y, dates, train_ratio=0.8, n_splits=3, window_size=30):\n",
    "    \"\"\"\n",
    "    전체 데이터의 80%까지를 초기 Train으로 하고,\n",
    "    이후를 n_splits 등분하여 Expanding Window 방식으로 테스트하는 예시입니다.\n",
    "    각 분할에서 모델 학습 시, 에포크마다 loss가 출력됩니다.\n",
    "    \"\"\"\n",
    "    n_total = len(X)\n",
    "    n_initial_train = int(n_total * train_ratio)\n",
    "\n",
    "    # 초반 80%를 기본 훈련 세트로 하고, 나머지 20%를 n_splits 구간으로 나눔\n",
    "    X_train_initial = X[:n_initial_train]\n",
    "    y_train_initial = y[:n_initial_train]\n",
    "    dates_train_initial = dates[:n_initial_train]\n",
    "\n",
    "    X_test_part = X[n_initial_train:]\n",
    "    y_test_part = y[n_initial_train:]\n",
    "    dates_test_part = dates[n_initial_train:]\n",
    "    \n",
    "    # n_splits로 나눔\n",
    "    chunk_size = len(X_test_part) // n_splits\n",
    "\n",
    "    results = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    # Expanding window 방식 (각 분할마다 누적된 데이터로 학습)\n",
    "    for split_idx in range(n_splits):\n",
    "        end_idx = start_idx + chunk_size\n",
    "        if split_idx == n_splits - 1:  # 마지막 구간 처리\n",
    "            end_idx = len(X_test_part)\n",
    "\n",
    "        # 현재 테스트 구간\n",
    "        X_test_current = X_test_part[start_idx:end_idx]\n",
    "        y_test_current = y_test_part[start_idx:end_idx]\n",
    "        dates_test_current = dates_test_part[start_idx:end_idx]\n",
    "\n",
    "        # Train 데이터: 초기 Train + 지금까지의 Test 일부\n",
    "        X_train_current = np.concatenate([X_train_initial, X_test_part[:start_idx]], axis=0)\n",
    "        y_train_current = np.concatenate([y_train_initial, y_test_part[:start_idx]], axis=0)\n",
    "\n",
    "        # 모델 학습 (verbose=1로 설정하여 각 에포크의 손실을 출력)\n",
    "        # 수정: X_all 대신 X_train_current의 피처 수 사용\n",
    "        model = build_lstm_model(input_shape=(window_size, X_train_current.shape[2]))\n",
    "        print(f\"\\n[Split {split_idx}] Training on {len(X_train_current)} samples; Testing from {dates_test_current[0]} to {dates_test_current[-1]}\")\n",
    "        history = model.fit(\n",
    "            X_train_current, y_train_current,\n",
    "            epochs=10, batch_size=32, verbose=1\n",
    "        )\n",
    "\n",
    "        # 예측 및 RMSE 계산\n",
    "        preds = model.predict(X_test_current)\n",
    "        mse = mean_squared_error(y_test_current, preds)\n",
    "        rmse = sqrt(mse)\n",
    "\n",
    "        results.append({\n",
    "            'split': split_idx,\n",
    "            'test_range': (dates_test_current[0], dates_test_current[-1]),\n",
    "            'rmse': rmse,\n",
    "            'history': history  # 원한다면 각 분할의 history도 저장 가능\n",
    "        })\n",
    "\n",
    "        start_idx = end_idx  # 다음 구간으로 이동\n",
    "\n",
    "    return results\n",
    "\n",
    "###################################\n",
    "# 7. LSTM(2레이어) 모델 구성 함수\n",
    "###################################\n",
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    2레이어 LSTM(64, 32) + Dropout(0.2) + Dense(1)\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(32, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "###################################\n",
    "# 8. Walk-Forward Validation 실행\n",
    "###################################\n",
    "results = walk_forward_validation(X_all, y_all, date_all, train_ratio=0.8, n_splits=3)\n",
    "\n",
    "###################################\n",
    "# 9. 결과 분석\n",
    "###################################\n",
    "for r in results:\n",
    "    print(f\"Split {r['split']} | Test Range: {r['test_range'][0]} ~ {r['test_range'][1]} | RMSE: {r['rmse']:.4f}\")\n",
    "\n",
    "avg_rmse = np.mean([r['rmse'] for r in results])\n",
    "print(f\"Average RMSE across splits: {avg_rmse:.4f}\")\n",
    "\n",
    "###################################\n",
    "# 10. 최종 모델 학습 (단순 분할)\n",
    "# 전체 데이터의 80%를 Train, 나머지 20%를 Test로 단순 분할\n",
    "simple_split = int(len(X_all) * 0.8)\n",
    "X_train_simple, X_test_simple = X_all[:simple_split], X_all[simple_split:]\n",
    "y_train_simple, y_test_simple = y_all[:simple_split], y_all[simple_split:]\n",
    "\n",
    "model_final = build_lstm_model((window_size, X_all.shape[2]))\n",
    "# validation_split=0.2를 추가하여, 학습 과정 중 검증 손실을 기록\n",
    "history_final = model_final.fit(X_train_simple, y_train_simple, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "preds_simple = model_final.predict(X_test_simple)\n",
    "rmse_simple = sqrt(mean_squared_error(y_test_simple, preds_simple))\n",
    "print(f\"[Simple final split] RMSE: {rmse_simple:.4f}\")\n",
    "\n",
    "# 학습 성능 시각화 (최종 모델)\n",
    "plt.plot(history_final.history['loss'], label='Training Loss')\n",
    "plt.plot(history_final.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Final Model Training/Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 실제 미래 값 예측\n",
    "\n",
    "\n",
    "def predict_future(model, last_window, steps):\n",
    "    \"\"\"\n",
    "    model: 학습된 모델 (예: model_final)\n",
    "    last_window: 마지막 시계열 윈도우 데이터, shape = (window_size, feature 수)\n",
    "    steps: 예측할 미래 시점의 수 (예: 5일)\n",
    "    \n",
    "    반환: 예측된 미래 'Close' 값들의 배열 (정규화된 값)\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    # 현재 윈도우를 복사 (매 반복마다 업데이트됨)\n",
    "    current_window = last_window.copy()\n",
    "    \n",
    "    for i in range(steps):\n",
    "        # 현재 윈도우에 대해 모델 예측 (입력 shape: (1, window_size, feature 수))\n",
    "        current_window_expanded = np.expand_dims(current_window, axis=0)\n",
    "        pred_norm = model.predict(current_window_expanded)[0, 0]  # 예측된 종가 (정규화된 값)\n",
    "        predictions.append(pred_norm)\n",
    "        \n",
    "        # 미래의 다른 피처(Volume, Sentiment)는 최근 관측치를 그대로 사용\n",
    "        # 새 행: 예측된 종가 + 마지막 행의 Volume, Sentiment 값 복사\n",
    "        new_row = current_window[-1].copy()\n",
    "        new_row[0] = pred_norm  # 종가는 예측값으로 대체\n",
    "        \n",
    "        # 슬라이딩 윈도우 업데이트: 기존 윈도우의 첫 행 제거하고 새 행 추가\n",
    "        current_window = np.vstack([current_window[1:], new_row])\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# 마지막 윈도우 데이터 (X_all에서 가장 마지막 샘플)\n",
    "last_window = X_all[-1]  # shape: (window_size, 3)\n",
    "\n",
    "# 예측할 미래 시점의 수 (예: 앞으로 5일)\n",
    "steps = 5\n",
    "future_predictions = predict_future(model_final, last_window, steps)\n",
    "print(\"Future predicted normalized 'Close' values:\", future_predictions)\n",
    "\n",
    "# 만약 원래 스케일(예: 달러 단위)로 변환하고 싶다면,\n",
    "# 예측값 배열을 [예측값, dummy, dummy] 형식으로 만들어 scaler.inverse_transform을 적용할 수 있습니다.\n",
    "# 예를 들어:\n",
    "dummy = np.zeros_like(future_predictions)\n",
    "pred_array = np.column_stack((future_predictions, dummy, dummy))\n",
    "# scaled_df에서 사용한 scaler를 적용 (단, 이 방법은 예시이며 실제로는 적절한 값 대체가 필요합니다)\n",
    "preds_original = scaler.inverse_transform(pred_array)[:, 0]\n",
    "print(\"Future predicted 'Close' values (original scale):\", preds_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측 + 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1. 데이터의 날짜 정보를 이용해 훈련/테스트 인덱스 선택\n",
    "# date_all는 create_sequences_multi_features 함수에서 반환된 날짜 리스트 (pandas DatetimeIndex 혹은 리스트)\n",
    "last_date = date_all[-1]  # 전체 시퀀스의 마지막 날짜\n",
    "\n",
    "# \"2주 전\"까지의 데이터를 훈련으로 사용하기 위한 종료 날짜: \n",
    "# 즉, 2주 전 이전의 시퀀스만 훈련에 포함\n",
    "train_end_date = last_date - pd.Timedelta(weeks=2)\n",
    "\n",
    "# 테스트 데이터는 \"마지막 주\"에 해당하는 시퀀스\n",
    "test_start_date = last_date - pd.Timedelta(weeks=1)  # 마지막 주 시작일\n",
    "test_end_date = last_date  # 마지막 날짜\n",
    "\n",
    "# 훈련 인덱스: 날짜가 train_end_date보다 이전인 경우 선택\n",
    "train_indices = [i for i, d in enumerate(date_all) if d < train_end_date]\n",
    "# 테스트 인덱스: 날짜가 test_start_date와 test_end_date 사이인 경우 선택\n",
    "test_indices = [i for i, d in enumerate(date_all) if (d >= test_start_date) and (d <= test_end_date)]\n",
    "\n",
    "print(f\"Train indices: {len(train_indices)} samples\")\n",
    "print(f\"Test indices: {len(test_indices)} samples\")\n",
    "\n",
    "# 2. 훈련/테스트 데이터 추출\n",
    "X_train_custom = X_all[train_indices]\n",
    "y_train_custom = y_all[train_indices]\n",
    "X_test_custom = X_all[test_indices]\n",
    "y_test_custom = y_all[test_indices]\n",
    "\n",
    "# 3. 모델 구성, 학습 및 예측 (이미 정의된 build_lstm_model 사용)\n",
    "model_custom = build_lstm_model(input_shape=(window_size, X_all.shape[2]))\n",
    "\n",
    "# 훈련: validation_split을 사용할 수도 있습니다.\n",
    "history_custom = model_custom.fit(X_train_custom, y_train_custom, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# 4. 테스트 데이터에 대한 예측\n",
    "preds_custom = model_custom.predict(X_test_custom)\n",
    "\n",
    "# 5. 성능 평가: RMSE 계산 (정규화된 값 기준)\n",
    "rmse_custom = sqrt(mean_squared_error(y_test_custom, preds_custom))\n",
    "print(f\"[Custom Split Prediction] RMSE: {rmse_custom:.4f}\")\n",
    "\n",
    "# 6. (선택) 예측값을 원래 스케일로 변환하고 싶다면:\n",
    "dummy = np.zeros((preds_custom.shape[0], 2))  # Volume, Sentiment 자리에 채워 넣을 더미 값\n",
    "pred_array = np.concatenate((preds_custom, dummy), axis=1)\n",
    "preds_original = scaler.inverse_transform(pred_array)[:, 0]\n",
    "print(\"Predicted 'Close' values (original scale):\", preds_original)\n",
    "\n",
    "# 테스트 데이터의 실제 y값(정규화된 값)을 원본 스케일로 변환\n",
    "# 여기서 y_test_custom은 정규화된 타깃 값입니다.\n",
    "dummy = np.zeros((len(y_test_custom), 2))  # 'Volume', 'Sentiment' 자리에 들어갈 더미 값 (0)\n",
    "y_test_array = np.column_stack((y_test_custom, dummy))\n",
    "y_test_original = scaler.inverse_transform(y_test_array)[:, 0]\n",
    "\n",
    "# 예측된 정규화된 y값(preds_custom)을 이미 원본 스케일로 변환한 preds_original을 사용합니다.\n",
    "# preds_original는 [230.95845902, 231.90000591, 232.90456854, 233.68290735, 234.17207753]로 출력됨\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(len(y_test_original)), y_test_original, marker='o', linestyle='-', color='blue', label='Actual Close')\n",
    "plt.plot(range(len(preds_original)), preds_original, marker='o', linestyle='--', color='red', label='Predicted Close')\n",
    "plt.xlabel('Test Sample (Day)')\n",
    "plt.ylabel('Close Price (Original Scale)')\n",
    "plt.title('Actual vs Predicted Close Prices (Last Week)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
